{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT)\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import (BatchNormalization,Flatten,Convolution1D,Activation,Input,Dense,LSTM)\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import losses, models, optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import itertools\n",
    "import warnings\n",
    "import librosa\n",
    "import pywt\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "The LANL Earthquake Prediction competition (https://www.kaggle.com/c/LANL-Earthquake-Prediction/overview) requires competitors to predict the time remaining (Time to failure, or TTF) until a laboratory earthquake occurs from real-time seismic data. We are given 150,000 data points of seismic data, which corresponds to 0.0375 seconds of seismic data (ordered in time).\n",
    "\n",
    "The first place solution to LANL Earthquake Prediction is a geometric mean of a neural network (NN) solution and LightGBM (LGBM) solution. Since the NN and LGBM algorithms are very different, they each capture different parts of the signal, and blending the two together increases generalization.\n",
    "\n",
    "To make predictions, we divide our training data into 150,000-length segments. Instead of using all of the training data, we decided to use only segments from the earthquake cycles that had exhibited higher TTF. This caused the TTF predictions to be biased higher.\n",
    "\n",
    "The raw acoustic data itself is noisy; therefore, we utilize various packages to denoise the signal. Additionally, we inject random noise to every segment and remove the median of the segment, because we noticed the mean & median values were increasing as the laboratory experiment went forward in time. This improves generalization.\n",
    "\n",
    "Additional solution details can be found on Kaggle's Discussion forum at https://www.kaggle.com/c/LANL-Earthquake-Prediction/discussion/94390."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw train data import\n",
    "raw = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for parsing and feature generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The normalize function is required to normalize the data for the neural network.\n",
    "\n",
    "def normalize(X_train, X_valid, X_test, normalize_opt, excluded_feat):\n",
    "    feats = [f for f in X_train.columns if f not in excluded_feat]\n",
    "    if normalize_opt != None:\n",
    "        if normalize_opt == 'min_max':\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "        elif normalize_opt == 'robust':\n",
    "            scaler = preprocessing.RobustScaler()\n",
    "        elif normalize_opt == 'standard':\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "        elif normalize_opt == 'max_abs':\n",
    "            scaler = preprocessing.MaxAbsScaler()\n",
    "        scaler = scaler.fit(X_train[feats])\n",
    "        X_train[feats] = scaler.transform(X_train[feats])\n",
    "        X_valid[feats] = scaler.transform(X_valid[feats])\n",
    "        X_test[feats] = scaler.transform(X_test[feats])\n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for feature generation\n",
    "# Create random noise for robustness\n",
    "np.random.seed(1337)\n",
    "noise = np.random.normal(0, 0.5, 150_000)\n",
    "\n",
    "# Mean Absolute Deviation\n",
    "def maddest(d, axis=None):\n",
    "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n",
    "\n",
    "# Denoise the raw signal given a segment x\n",
    "def denoise_signal(x, wavelet='db4', level=1):\n",
    "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
    "    sigma = (1/0.6745) * maddest(coeff[-level])\n",
    "    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
    "\n",
    "    return pywt.waverec(coeff, wavelet, mode='per')\n",
    "\n",
    "# Denoise the raw signal (simplified) given a segment x\n",
    "def denoise_signal_simple(x, wavelet='db4', level=1):\n",
    "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
    "    #univeral threshold\n",
    "    uthresh = 10\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
    "    # Reconstruct the signal using the thresholded coefficients\n",
    "    return pywt.waverec(coeff, wavelet, mode='per')\n",
    "\n",
    "# Generate the features given a segment z\n",
    "def feature_gen(z):\n",
    "    X = pd.DataFrame(index=[0], dtype=np.float64)\n",
    "    \n",
    "    # Add noise, subtract median to remove bias from mean/median as time passes in the experiment\n",
    "    # Save the result as a new segment, z\n",
    "    z = z + noise\n",
    "    z = z - np.median(z)\n",
    "\n",
    "    # Save denoised versions of z\n",
    "    den_sample = denoise_signal(z)\n",
    "    den_sample_simple = denoise_signal_simple(z)\n",
    "    \n",
    "    # Mel-frequency cepstral coefficients\n",
    "    mfcc = librosa.feature.mfcc(z)\n",
    "    mfcc_mean = mfcc.mean(axis=1)\n",
    "    mfcc_denoise_simple = librosa.feature.mfcc(den_sample_simple)\n",
    "    mfcc_mean_denoise_simple = mfcc_denoise_simple.mean(axis=1) #0-19\n",
    "    \n",
    "    # Spectral contrast\n",
    "    lib_spectral_contrast_denoise_simple = librosa.feature.spectral_contrast(den_sample_simple).mean(axis=1) #0-6\n",
    "    lib_spectral_contrast = librosa.feature.spectral_contrast(z).mean(axis=1) #0-6\n",
    "    \n",
    "    # Neural network features\n",
    "    X['NN_zero_crossings_denoise'] = len(np.where(np.diff(np.sign(den_sample)))[0])\n",
    "    X['NN_LGBM_percentile_roll20_std_50'] = np.percentile(pd.Series(z).rolling(20).std().dropna().values, 50)\n",
    "    X['NN_q95_roll20_std'] = np.quantile(pd.Series(z).rolling(20).std().dropna().values, 0.95)\n",
    "    X['NN_LGBM_mfcc_mean4'] = mfcc_mean[4]\n",
    "    X['NN_lib_spectral_contrast0'] = lib_spectral_contrast[0]\n",
    "    X['NN_num_peaks_3_denoise'] = feature_calculators.number_peaks(den_sample, 3)\n",
    "    X['NN_mfcc_mean_denoise_simple2'] = mfcc_mean_denoise_simple[2]\n",
    "    X['NN_mfcc_mean5'] = mfcc_mean[5]\n",
    "    X['NN_mfcc_mean2'] = mfcc_mean[2]\n",
    "    X['NN_mfcc_mean_denoise_simple5'] = mfcc_mean_denoise_simple[5]\n",
    "    X['NN_absquant95'] = np.quantile(np.abs(z), 0.95)\n",
    "    X['NN_median_roll50_std_denoise_simple'] = np.median(pd.Series(den_sample_simple).rolling(50).std().dropna().values)\n",
    "    X['NN_mfcc_mean_denoise_simple1'] = mfcc_mean_denoise_simple[1]\n",
    "    X['NN_quant99'] = np.quantile(z, 0.99)\n",
    "    X['NN_lib_zero_cross_rate_denoise_simple'] = librosa.feature.zero_crossing_rate(den_sample_simple)[0].mean()\n",
    "    X['NN_fftr_max_denoise'] = np.max(pd.Series(np.abs(np.fft.fft(den_sample)))[0:75000])\n",
    "    X['NN_abssumgreater15'] = np.sum(abs(z[np.where(abs(z)>15)]))\n",
    "    X['NN_LGBM_mfcc_mean18'] = mfcc_mean[18]\n",
    "    X['NN_lib_spectral_contrast_denoise_simple2'] = lib_spectral_contrast_denoise_simple[2]\n",
    "    X['NN_fftr_sum'] = np.sum(pd.Series(np.abs(np.fft.fft(z)))[0:75000])\n",
    "    X['NN_mfcc_mean_denoise_simple10'] = mfcc_mean_denoise_simple[10]\n",
    "    \n",
    "    # Extra features only LGBM used.\n",
    "    X['LGBM_num_peaks_2_denoise_simple'] = feature_calculators.number_peaks(den_sample_simple, 2)\n",
    "    X['LGBM_autocorr5'] = feature_calculators.autocorrelation(pd.Series(z), 5)\n",
    "    \n",
    "    # Windowed fast fourier transformations\n",
    "    fftrhann20000 = np.sum(np.abs(np.fft.fft(np.hanning(len(z))*z)[:20000]))\n",
    "    fftrhann20000_denoise = np.sum(np.abs(np.fft.fft(np.hanning(len(z))*den_sample)[:20000]))\n",
    "    fftrhann20000_diff_rate = (fftrhann20000 - fftrhann20000_denoise)/fftrhann20000\n",
    "    \n",
    "    X['LGBM_fftrhann20000_diff_rate'] = fftrhann20000_diff_rate\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0938abe10444f9b56b5af4474227f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4194), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train and test sets\n",
    "def parse_sample(sample, start):\n",
    "    delta = feature_gen(sample['acoustic_data'].values)\n",
    "    delta['start'] = start\n",
    "    delta['target'] = sample['time_to_failure'].values[-1]\n",
    "    return delta\n",
    "    \n",
    "def sample_train_gen(df, segment_size=150_000, indices_to_calculate=[0]):\n",
    "    result = Parallel(n_jobs=1, temp_folder=\"/tmp\", max_nbytes=None, backend=\"multiprocessing\")(delayed(parse_sample)(df[int(i) : int(i) + segment_size], int(i)) \n",
    "                                                                                                for i in tqdm(indices_to_calculate))\n",
    "    data = [r.values for r in result]\n",
    "    data = np.vstack(data)\n",
    "    X = pd.DataFrame(data, columns=result[0].columns)\n",
    "    X = X.sort_values(\"start\")\n",
    "    return X\n",
    "\n",
    "def parse_sample_test(seg_id):\n",
    "    sample = pd.read_csv('../input/test/' + seg_id + '.csv', dtype={'acoustic_data': np.int32})\n",
    "    delta = feature_gen(sample['acoustic_data'].values)\n",
    "    delta['seg_id'] = seg_id\n",
    "    return delta\n",
    "\n",
    "def sample_test_gen():\n",
    "    X = pd.DataFrame()\n",
    "    submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
    "    result = Parallel(n_jobs=1, temp_folder=\"/tmp\", max_nbytes=None, backend=\"multiprocessing\")(delayed(parse_sample_test)(seg_id) for seg_id in tqdm(submission.index))\n",
    "    data = [r.values for r in result]\n",
    "    data = np.vstack(data)\n",
    "    X = pd.DataFrame(data, columns=result[0].columns)\n",
    "    return X\n",
    "\n",
    "indices_to_calculate = raw.index.values[::150_000][:-1]\n",
    "\n",
    "train = sample_train_gen(raw, indices_to_calculate=indices_to_calculate)\n",
    "del raw\n",
    "gc.collect()\n",
    "test = sample_test_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep CV observations from train set\n",
    "etq_meta = [\n",
    "{\"start\":0,         \"end\":5656574},\n",
    "{\"start\":5656574,   \"end\":50085878},\n",
    "{\"start\":50085878,  \"end\":104677356},\n",
    "{\"start\":104677356, \"end\":138772453},\n",
    "{\"start\":138772453, \"end\":187641820},\n",
    "{\"start\":187641820, \"end\":218652630},\n",
    "{\"start\":218652630, \"end\":245829585},\n",
    "{\"start\":245829585, \"end\":307838917},\n",
    "{\"start\":307838917, \"end\":338276287},\n",
    "{\"start\":338276287, \"end\":375377848},\n",
    "{\"start\":375377848, \"end\":419368880},\n",
    "{\"start\":419368880, \"end\":461811623},\n",
    "{\"start\":461811623, \"end\":495800225},\n",
    "{\"start\":495800225, \"end\":528777115},\n",
    "{\"start\":528777115, \"end\":585568144},\n",
    "{\"start\":585568144, \"end\":621985673},\n",
    "{\"start\":621985673, \"end\":629145480},\n",
    "]\n",
    "\n",
    "for i, etq in enumerate(etq_meta):\n",
    "    train.loc[(train['start'] + 150_000 >= etq[\"start\"]) & (train['start'] <= etq[\"end\"] - 150_000), \"eq\"] = i\n",
    "\n",
    "# We are only keeping segments that belong in these earthquakes\n",
    "# This is to make the training distribution more like the testing distribution\n",
    "train_sample = train[train[\"eq\"].isin([2, 7, 0, 4, 11, 13, 9, 1, 14, 10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete unnecessary files\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index of the final train set\n",
    "train_sample=train_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5fdf9f458d438ea8eb5cddf39928af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2857), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create time since failure target variable\n",
    "# This will be used in the NN as an additional objective\n",
    "targets=train_sample[['target','start']]\n",
    "targets['tsf']=targets['target']-targets['target'].shift(1).fillna(0)\n",
    "targets['tsf']=np.where(targets['tsf']>1.5, targets['tsf'], 0)\n",
    "targets['tsf'].iloc[0]=targets['target'].iloc[0]\n",
    "\n",
    "temp_max=0\n",
    "for i in tqdm(range(targets.shape[0])):\n",
    "    if targets['tsf'].iloc[i]>0:\n",
    "        temp_max=targets['tsf'].iloc[i]\n",
    "    else:\n",
    "        targets['tsf'].iloc[i]=temp_max\n",
    "        \n",
    "targets['tsf']=targets['tsf']-targets['target']\n",
    "\n",
    "# create a flag target variable for TTF<0.5 secs\n",
    "# This will be used in the NN as an additional objective\n",
    "target=targets['target'].copy().values\n",
    "target[target>=0.5]=1\n",
    "target[target<0.5]=0\n",
    "target=1-target\n",
    "\n",
    "targets['binary']=target\n",
    "del target\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import submission file\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unnecessary columns\n",
    "train_sample.drop(['start', 'target', 'eq'],axis=1,inplace=True)\n",
    "test.drop(['seg_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to convert test columns from objects to float64\n",
    "test = test.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your kfold cross validation\n",
    "# We used 3 folds, because we did not see improvements with higher folds\n",
    "# We are not scared of shuffling because The whole point of this comp is to be independent of time. Test is shuffled\n",
    "n_fold = 3\n",
    "\n",
    "kf = KFold(n_splits=n_fold, shuffle=True, random_state=1337)\n",
    "kf = list(kf.split(np.arange(len(train_sample))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the LGBM\n",
    "\n",
    "The LGBM is trained using only six features. We train using shuffled 3Fold. The LGBM is averaged over ten runs to improve generalization. Hyperparameters were optimized for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features LGBM is using are: ['NN_LGBM_percentile_roll20_std_50', 'NN_LGBM_mfcc_mean4', 'NN_LGBM_mfcc_mean18', 'LGBM_num_peaks_2_denoise_simple', 'LGBM_autocorr5', 'LGBM_fftrhann20000_diff_rate']\n"
     ]
    }
   ],
   "source": [
    "LGBM_feats = [feat for feat in train_sample.columns if 'LGBM' in feat]\n",
    "print('The features LGBM is using are:', LGBM_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.65541\tvalid_1's l1: 1.96756\n",
      "[2000]\ttraining's l1: 1.54143\tvalid_1's l1: 1.96991\n",
      "Early stopping, best iteration is:\n",
      "[1722]\ttraining's l1: 1.5693\tvalid_1's l1: 1.96521\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.71743\tvalid_1's l1: 1.84093\n",
      "Early stopping, best iteration is:\n",
      "[811]\ttraining's l1: 1.74917\tvalid_1's l1: 1.83856\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.6923\tvalid_1's l1: 1.88817\n",
      "[2000]\ttraining's l1: 1.5755\tvalid_1's l1: 1.88647\n",
      "Early stopping, best iteration is:\n",
      "[1476]\ttraining's l1: 1.63038\tvalid_1's l1: 1.87942\n",
      "Seed 1\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.64907\tvalid_1's l1: 1.9683\n",
      "[2000]\ttraining's l1: 1.53707\tvalid_1's l1: 1.97264\n",
      "Early stopping, best iteration is:\n",
      "[1005]\ttraining's l1: 1.64879\tvalid_1's l1: 1.96717\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.72068\tvalid_1's l1: 1.84038\n",
      "Early stopping, best iteration is:\n",
      "[636]\ttraining's l1: 1.78197\tvalid_1's l1: 1.83108\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.69042\tvalid_1's l1: 1.88571\n",
      "[2000]\ttraining's l1: 1.56874\tvalid_1's l1: 1.89375\n",
      "Early stopping, best iteration is:\n",
      "[1440]\ttraining's l1: 1.62893\tvalid_1's l1: 1.88064\n",
      "Seed 2\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.65686\tvalid_1's l1: 1.97314\n",
      "[2000]\ttraining's l1: 1.54261\tvalid_1's l1: 1.98057\n",
      "Early stopping, best iteration is:\n",
      "[1147]\ttraining's l1: 1.63633\tvalid_1's l1: 1.97074\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.71949\tvalid_1's l1: 1.84798\n",
      "Early stopping, best iteration is:\n",
      "[717]\ttraining's l1: 1.76896\tvalid_1's l1: 1.84099\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.69373\tvalid_1's l1: 1.89275\n",
      "[2000]\ttraining's l1: 1.57514\tvalid_1's l1: 1.8955\n",
      "Early stopping, best iteration is:\n",
      "[1440]\ttraining's l1: 1.63424\tvalid_1's l1: 1.88547\n",
      "Seed 3\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.65622\tvalid_1's l1: 1.97481\n",
      "[2000]\ttraining's l1: 1.54359\tvalid_1's l1: 1.97615\n",
      "Early stopping, best iteration is:\n",
      "[1724]\ttraining's l1: 1.56993\tvalid_1's l1: 1.9716\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.71885\tvalid_1's l1: 1.83778\n",
      "Early stopping, best iteration is:\n",
      "[722]\ttraining's l1: 1.76707\tvalid_1's l1: 1.83145\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.69892\tvalid_1's l1: 1.89058\n",
      "[2000]\ttraining's l1: 1.5842\tvalid_1's l1: 1.89019\n",
      "Early stopping, best iteration is:\n",
      "[1512]\ttraining's l1: 1.63387\tvalid_1's l1: 1.88313\n",
      "Seed 4\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.65056\tvalid_1's l1: 1.97832\n",
      "[2000]\ttraining's l1: 1.53736\tvalid_1's l1: 1.97545\n",
      "Early stopping, best iteration is:\n",
      "[1753]\ttraining's l1: 1.56248\tvalid_1's l1: 1.97289\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.72311\tvalid_1's l1: 1.83928\n",
      "Early stopping, best iteration is:\n",
      "[630]\ttraining's l1: 1.78817\tvalid_1's l1: 1.83311\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.69194\tvalid_1's l1: 1.89073\n",
      "[2000]\ttraining's l1: 1.57514\tvalid_1's l1: 1.88859\n",
      "Early stopping, best iteration is:\n",
      "[1470]\ttraining's l1: 1.62987\tvalid_1's l1: 1.88241\n",
      "Seed 5\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.65047\tvalid_1's l1: 1.98083\n",
      "Early stopping, best iteration is:\n",
      "[634]\ttraining's l1: 1.70959\tvalid_1's l1: 1.97581\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.7189\tvalid_1's l1: 1.84977\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttraining's l1: 1.76996\tvalid_1's l1: 1.8419\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.69392\tvalid_1's l1: 1.88168\n",
      "[2000]\ttraining's l1: 1.57459\tvalid_1's l1: 1.88241\n",
      "Early stopping, best iteration is:\n",
      "[1440]\ttraining's l1: 1.63397\tvalid_1's l1: 1.87356\n",
      "Seed 6\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.65295\tvalid_1's l1: 1.98279\n",
      "Early stopping, best iteration is:\n",
      "[689]\ttraining's l1: 1.70144\tvalid_1's l1: 1.97883\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.71898\tvalid_1's l1: 1.84304\n",
      "Early stopping, best iteration is:\n",
      "[681]\ttraining's l1: 1.77256\tvalid_1's l1: 1.83931\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.69454\tvalid_1's l1: 1.88817\n",
      "[2000]\ttraining's l1: 1.57539\tvalid_1's l1: 1.89401\n",
      "Early stopping, best iteration is:\n",
      "[1440]\ttraining's l1: 1.63486\tvalid_1's l1: 1.88433\n",
      "Seed 7\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.65393\tvalid_1's l1: 1.97501\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttraining's l1: 1.70521\tvalid_1's l1: 1.97408\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.72042\tvalid_1's l1: 1.84324\n",
      "Early stopping, best iteration is:\n",
      "[491]\ttraining's l1: 1.81274\tvalid_1's l1: 1.83444\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.68734\tvalid_1's l1: 1.89409\n",
      "[2000]\ttraining's l1: 1.56856\tvalid_1's l1: 1.88781\n",
      "Early stopping, best iteration is:\n",
      "[1734]\ttraining's l1: 1.59556\tvalid_1's l1: 1.88142\n",
      "Seed 8\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.65161\tvalid_1's l1: 1.97684\n",
      "Early stopping, best iteration is:\n",
      "[839]\ttraining's l1: 1.67667\tvalid_1's l1: 1.9765\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.71893\tvalid_1's l1: 1.84541\n",
      "Early stopping, best iteration is:\n",
      "[718]\ttraining's l1: 1.76676\tvalid_1's l1: 1.83729\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.69257\tvalid_1's l1: 1.90039\n",
      "[2000]\ttraining's l1: 1.57566\tvalid_1's l1: 1.8956\n",
      "Early stopping, best iteration is:\n",
      "[1798]\ttraining's l1: 1.59565\tvalid_1's l1: 1.89007\n",
      "Seed 9\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.65325\tvalid_1's l1: 1.98241\n",
      "Early stopping, best iteration is:\n",
      "[586]\ttraining's l1: 1.72069\tvalid_1's l1: 1.9762\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.719\tvalid_1's l1: 1.83863\n",
      "Early stopping, best iteration is:\n",
      "[603]\ttraining's l1: 1.79064\tvalid_1's l1: 1.83357\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's l1: 1.69149\tvalid_1's l1: 1.89138\n",
      "[2000]\ttraining's l1: 1.57362\tvalid_1's l1: 1.88754\n",
      "Early stopping, best iteration is:\n",
      "[1460]\ttraining's l1: 1.63263\tvalid_1's l1: 1.88138\n",
      "\n",
      "MAE for LGBM:  1.8937914414723107\n"
     ]
    }
   ],
   "source": [
    "oof_LGBM = np.zeros(len(train_sample))\n",
    "sub_LGBM = np.zeros(len(submission))\n",
    "seeds = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "for seed in seeds:\n",
    "    print('Seed',seed)\n",
    "    for fold_n, (train_index, valid_index) in enumerate(kf):\n",
    "        print('Fold', fold_n)\n",
    "\n",
    "        # Create train and validation data using only LGBM_feats.\n",
    "        trn_data = lgb.Dataset(train_sample[LGBM_feats].iloc[train_index], label=targets['target'].iloc[train_index])\n",
    "        val_data = lgb.Dataset(train_sample[LGBM_feats].iloc[valid_index], label=targets['target'].iloc[valid_index])\n",
    "\n",
    "        params = {'num_leaves': 4, # Low number of leaves reduces LGBM complexity\n",
    "          'min_data_in_leaf': 5,\n",
    "          'objective':'fair', # Fitting to fair objective performed better than fitting to MAE objective\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting\": \"gbdt\", \n",
    "          'boost_from_average': True,\n",
    "          \"feature_fraction\": 0.9,\n",
    "          \"bagging_freq\": 1,\n",
    "          \"bagging_fraction\": 0.5,\n",
    "          \"bagging_seed\": 0,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'max_bin': 500,\n",
    "          'reg_alpha': 0, \n",
    "          'reg_lambda': 0,\n",
    "          'seed': seed,\n",
    "          'n_jobs': 1\n",
    "          }\n",
    "\n",
    "        clf = lgb.train(params, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n",
    "\n",
    "        oof_LGBM[valid_index] += clf.predict(train_sample[LGBM_feats].iloc[valid_index], num_iteration=clf.best_iteration)\n",
    "        sub_LGBM += clf.predict(test[LGBM_feats], num_iteration=clf.best_iteration) / n_fold\n",
    "        \n",
    "oof_LGBM = oof_LGBM / len(seeds)\n",
    "sub_LGBM = sub_LGBM / len(seeds)\n",
    "    \n",
    "print('\\nMAE for LGBM: ', mean_absolute_error(targets['target'], oof_LGBM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NN\n",
    "\n",
    "The NN is trained using shuffled 3Fold. It is averaged over 8 runs to improve generalization. Sometimes when running the model, the initial weights are bad which results in bad results in cross-validation. If this happens, we will not use it when we average.\n",
    "\n",
    "The model is simultaneously fit to three targets: Time to Failure (TTF), Time Since Failure (TSF), and Binary Target for TTF < 0.5 seconds. The loss weights are 8, 1, and 1, respectively. Because the NN has to focus on the TSF and Binary targets, the weights created seem to be better for predicting TTF. Likely, by fitting the NN this way, it reduces overfitting and increases generalization.\n",
    "\n",
    "We use Nadam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features NN is using are: ['NN_zero_crossings_denoise', 'NN_LGBM_percentile_roll20_std_50', 'NN_q95_roll20_std', 'NN_LGBM_mfcc_mean4', 'NN_lib_spectral_contrast0', 'NN_num_peaks_3_denoise', 'NN_mfcc_mean_denoise_simple2', 'NN_mfcc_mean5', 'NN_mfcc_mean2', 'NN_mfcc_mean_denoise_simple5', 'NN_absquant95', 'NN_median_roll50_std_denoise_simple', 'NN_mfcc_mean_denoise_simple1', 'NN_quant99', 'NN_lib_zero_cross_rate_denoise_simple', 'NN_fftr_max_denoise', 'NN_abssumgreater15', 'NN_LGBM_mfcc_mean18', 'NN_lib_spectral_contrast_denoise_simple2', 'NN_fftr_sum', 'NN_mfcc_mean_denoise_simple10']\n"
     ]
    }
   ],
   "source": [
    "NN_feats = [feat for feat in train_sample.columns if 'NN' in feat]\n",
    "print('The features NN is using are:', NN_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset columns to only use the neural network features\n",
    "train_sample = train_sample[NN_feats]\n",
    "test = test[NN_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Neural Network architecture\n",
    "def get_model():\n",
    "\n",
    "    inp = Input(shape=(1,train_sample.shape[1]))\n",
    "    x = BatchNormalization()(inp)\n",
    "    x = LSTM(128,return_sequences=True)(x) # LSTM as first layer performed better than Dense.\n",
    "    x = Convolution1D(128, (2),activation='relu', padding=\"same\")(x)\n",
    "    x = Convolution1D(84, (2),activation='relu', padding=\"same\")(x)\n",
    "    x = Convolution1D(64, (2),activation='relu', padding=\"same\")(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    \n",
    "    #outputs\n",
    "    ttf = Dense(1, activation='relu',name='regressor')(x) # Time to Failure\n",
    "    tsf = Dense(1)(x) # Time Since Failure\n",
    "    classifier = Dense(1, activation='sigmoid')(x) # Binary for TTF<0.5 seconds\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=[ttf,tsf,classifier])    \n",
    "    opt = optimizers.Nadam(lr=0.008)\n",
    "\n",
    "    # We are fitting to 3 targets simultaneously: Time to Failure (TTF), Time Since Failure (TSF), and Binary for TTF<0.5 seconds\n",
    "    # We weight the model to optimize heavily for TTF\n",
    "    # Optimizing for TSF and Binary TTF<0.5 helps to reduce overfitting, and helps for generalization.\n",
    "    model.compile(optimizer=opt, loss=['mae','mae','binary_crossentropy'],loss_weights=[8,1,1],metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model  1\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "MAE:  1.8645775062657923  Averaged\n",
      "Running Model  2\n",
      "MAE:  1.8614127821713429  Averaged\n",
      "Running Model  3\n",
      "MAE:  1.8737826835591311  Averaged\n",
      "Running Model  4\n",
      "MAE:  1.8684662884669454  Averaged\n",
      "Running Model  5\n",
      "MAE:  1.879080055511824  Averaged\n",
      "Running Model  6\n",
      "MAE:  1.872343232819506  Averaged\n",
      "Running Model  7\n",
      "MAE:  3.3600689398845556  Not Averaged\n",
      "Running Model  7\n",
      "MAE:  1.8624218971654969  Averaged\n",
      "Running Model  8\n",
      "MAE:  3.2885288286595262  Not Averaged\n",
      "Running Model  8\n",
      "MAE:  1.8738747517396408  Averaged\n",
      "\n",
      "MAE for NN:  1.8614309286259605\n"
     ]
    }
   ],
   "source": [
    "n=8 # number of NN runs\n",
    "\n",
    "oof_final = np.zeros(len(train_sample))\n",
    "sub_final = np.zeros(len(submission))\n",
    "i=0\n",
    "\n",
    "\n",
    "while i<8:\n",
    "    print('Running Model ', i+1)\n",
    "    \n",
    "    oof = np.zeros(len(train_sample))\n",
    "    prediction = np.zeros(len(submission))\n",
    "\n",
    "    for fold_n, (train_index, valid_index) in enumerate(kf):\n",
    "        #define training and validation sets\n",
    "\n",
    "        train_x=train_sample.iloc[train_index] #training set\n",
    "        train_y_ttf=targets['target'].iloc[train_index] #training target(Time to Failure)\n",
    "\n",
    "        valid_x=train_sample.iloc[valid_index] #validation set\n",
    "        valid_y_ttf=targets['target'].iloc[valid_index] #validation target(Time to Failure)\n",
    "\n",
    "        train_y_tsf=targets['tsf'].iloc[train_index] #training target(Time Since Failure)\n",
    "        train_y_clf=targets['binary'].iloc[train_index] #training target(Binary for TTF<0.5 Secs)\n",
    "\n",
    "        valid_y_tsf=targets['tsf'].iloc[valid_index] #validation target(Time Since Failure)\n",
    "        valid_y_clf=targets['binary'].iloc[valid_index] #validation target(Binary for TTF<0.5 Secs)\n",
    "\n",
    "        #apply min max scaler on training, validation data\n",
    "        train_x,valid_x,test_scaled=normalize(train_x.copy(), valid_x.copy(), test.copy(), 'min_max', [])\n",
    "\n",
    "        #Reshape training,validation,test data for fitting\n",
    "        train_x=train_x.values.reshape(train_x.shape[0],1,train_x.shape[1])\n",
    "        valid_x=valid_x.values.reshape(valid_x.shape[0],1,valid_x.shape[1])\n",
    "        test_scaled=test_scaled.values.reshape(test_scaled.shape[0],1,test_scaled.shape[1])\n",
    "\n",
    "        #obtain Neural Network Instance\n",
    "        model=get_model()\n",
    "\n",
    "        #setup Neural Network callbacks\n",
    "        cb_checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='val_regressor_mean_absolute_error', save_weights_only=True,save_best_only=True, period=1)\n",
    "        cb_Early_Stop=EarlyStopping( monitor='val_regressor_mean_absolute_error',patience=20)\n",
    "        cb_Reduce_LR = ReduceLROnPlateau(monitor='val_regressor_mean_absolute_error', factor=0.5, patience=5, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "        callbacks = [cb_checkpoint,cb_Early_Stop,cb_Reduce_LR] #define callbacks set\n",
    "        \n",
    "        ### NN seeds setup- Start\n",
    "        os.environ['PYTHONHASHSEED'] = '0'\n",
    "        np.random.seed(1234)\n",
    "        rn.seed(1234)\n",
    "        tf.set_random_seed(1234)\n",
    "        session_conf = tf.ConfigProto( allow_soft_placement=True)\n",
    "        sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "        K.set_session(sess)\n",
    "        ### NN seeds setup- End\n",
    "        \n",
    "        \n",
    "        model.fit(train_x,[train_y_ttf,train_y_tsf,train_y_clf],\n",
    "                  epochs=1000,callbacks=callbacks\n",
    "                  ,batch_size=256,verbose=0,\n",
    "                  validation_data=(valid_x,[valid_y_ttf,valid_y_tsf,valid_y_clf]))\n",
    "\n",
    "        model.load_weights(\"model.hdf5\")\n",
    "        \n",
    "        oof[valid_index] += model.predict(valid_x)[0].ravel()\n",
    "        prediction += model.predict(test_scaled)[0].ravel()/n_fold\n",
    "        \n",
    "        K.clear_session()\n",
    "        \n",
    "    # Obtain the MAE for this run.\n",
    "    model_score=mean_absolute_error(targets['target'], oof)\n",
    "    \n",
    "    # Sometimes, the NN performs very badly. This happens if the weights are initialized poorly.\n",
    "    # If the MAE is < 2, then the model has performed correctly, and we will use it in the average.\n",
    "    if model_score < 2:\n",
    "        print('MAE: ', model_score,' Averaged')\n",
    "        oof_final += oof/n\n",
    "        sub_final += prediction/n\n",
    "        i+=1 # Increase i, so we know that we completed a successful run.\n",
    "        \n",
    "    # If the MAE is >= 2, then the NN has performed badly.\n",
    "    # We will reject this run in the average.\n",
    "    else:\n",
    "        print('MAE: ', model_score,' Not Averaged')\n",
    "\n",
    "print('\\nMAE for NN: ', mean_absolute_error(targets['target'], oof_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine NN and LGBM using geometric mean\n",
    "\n",
    "The geometric mean requires taking product of N oofs, and then taking root(N) of the product. \n",
    "\n",
    "Since we have two oofs, we multiply the two, then take square root.\n",
    "\n",
    "We found the geometric mean to perform slightly better than mean and median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square root requires non-negative values, so let us force minima to something small.\n",
    "MIN_VALUE = 0.1\n",
    "\n",
    "# Correct LGBM predictions\n",
    "oof_LGBM[oof_LGBM < MIN_VALUE] = MIN_VALUE\n",
    "sub_LGBM[sub_LGBM < MIN_VALUE] = MIN_VALUE\n",
    "\n",
    "# Correct NN predictions\n",
    "oof_final[oof_final < MIN_VALUE] = MIN_VALUE\n",
    "sub_final[sub_final < MIN_VALUE] = MIN_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for LGBM was:  1.893283332657955\n",
      "MAE for NN was  :  1.8613892562909389\n",
      "\n",
      "MAE for geometric mean of LGBM and NN was :  1.8514547166080866\n"
     ]
    }
   ],
   "source": [
    "print('MAE for LGBM was: ', mean_absolute_error(targets['target'], oof_LGBM))\n",
    "print('MAE for NN was  : ', mean_absolute_error(targets['target'], oof_final))\n",
    "\n",
    "oof_geomean = (oof_LGBM * oof_final) ** (1/2)\n",
    "sub_geomean = (sub_LGBM * sub_final) ** (1/2)\n",
    "\n",
    "print('\\nMAE for geometric mean of LGBM and NN was : ', mean_absolute_error(targets['target'], oof_geomean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save oof and sub as numpy arrays\n",
    "np.save('oof_LGBM.npy', oof_LGBM)\n",
    "np.save('sub_LGBM.npy', sub_LGBM)\n",
    "np.save('oof_NN.npy', oof_final)\n",
    "np.save('sub_NN.npy', sub_final)\n",
    "np.save('oof_geomean.npy', oof_geomean)\n",
    "np.save('sub_geomean.npy', sub_geomean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            time_to_failure\n",
      "seg_id                     \n",
      "seg_00030f         3.431568\n",
      "seg_0012b5         5.762004\n",
      "seg_00184e         7.411745\n",
      "seg_003339         9.952178\n",
      "seg_0042cc         7.806593\n"
     ]
    }
   ],
   "source": [
    "# Save out the geometric mean submission\n",
    "submission['time_to_failure'] = sub_geomean\n",
    "print(submission.head())\n",
    "submission.to_csv('submission_geomean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            time_to_failure\n",
      "seg_id                     \n",
      "seg_00030f         4.011660\n",
      "seg_0012b5         5.742004\n",
      "seg_00184e         7.099880\n",
      "seg_003339         9.896426\n",
      "seg_0042cc         7.948604\n"
     ]
    }
   ],
   "source": [
    "# Save solution using just LGBM\n",
    "submission['time_to_failure'] = sub_LGBM\n",
    "print(submission.head())\n",
    "submission.to_csv('submission_LGBM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            time_to_failure\n",
      "seg_id                     \n",
      "seg_00030f         2.935359\n",
      "seg_0012b5         5.782074\n",
      "seg_00184e         7.737309\n",
      "seg_003339        10.008243\n",
      "seg_0042cc         7.667119\n"
     ]
    }
   ],
   "source": [
    "# Save solution using just NN\n",
    "submission['time_to_failure'] = sub_final\n",
    "print(submission.head())\n",
    "submission.to_csv('submission_NN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
